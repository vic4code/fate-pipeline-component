{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Data From Host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## two_party_continual_input_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 設定資料路徑 & 參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "guest, host = 9999, 10000\n",
    "data_base = \"/data/projects/fate/\"\n",
    "\n",
    "dense_data = {\"name\": \"motor_hetero_host\", \"namespace\": f\"experiment\"}\n",
    "dense_data_dir = os.path.join(data_base, \"persistence/data/motor_hetero_host.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缺失值 & 欄位名\n",
    "\n",
    "#### 欄位名請全部調整成小寫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dense_df = pd.read_csv(dense_data_dir)\n",
    "print(dense_df.isna().sum())\n",
    "print(dense_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上傳資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.backend.pipeline import PipeLine\n",
    "pipeline_upload = PipeLine().set_initiator(role='guest', party_id=guest).set_roles(guest=guest)\n",
    "partition = 4\n",
    "\n",
    "pipeline_upload.add_upload_data(file=dense_data_dir,\n",
    "                                table_name=dense_data[\"name\"],             # table name\n",
    "                                namespace=dense_data[\"namespace\"],         # namespace\n",
    "                                head=1, partition=partition)               # data info\n",
    "pipeline_upload.upload(drop=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## two_party_continual_input_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 設定資料路徑 & 參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "guest, host = 9999, 10000\n",
    "data_base = \"/data/projects/fate/\"\n",
    "\n",
    "dense_data = {\"name\": \"breast_hetero_host\", \"namespace\": f\"experiment\"}\n",
    "dense_data_dir = os.path.join(data_base, \"persistence/data/breast_hetero_host.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缺失值 & 欄位名\n",
    "\n",
    "#### 欄位名請全部調整成小寫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id     0\n",
      "x0     0\n",
      "x1     0\n",
      "x2     0\n",
      "x3     0\n",
      "x4     0\n",
      "x5     0\n",
      "x6     0\n",
      "x7     0\n",
      "x8     0\n",
      "x9     0\n",
      "x10    0\n",
      "x11    0\n",
      "x12    0\n",
      "x13    0\n",
      "x14    0\n",
      "x15    0\n",
      "x16    0\n",
      "x17    0\n",
      "x18    0\n",
      "x19    0\n",
      "dtype: int64\n",
      "    id        x0        x1        x2        x3        x4        x5        x6  \\\n",
      "0  133  0.449512 -1.247226  0.413178  0.303781 -0.123848 -0.184227 -0.219076   \n",
      "1  273 -1.245485 -0.842317 -1.255026 -1.038066 -0.426301 -1.088781 -0.976392   \n",
      "2  175 -1.549664 -1.126219 -1.546652 -1.216392 -0.354424 -1.167051 -1.114873   \n",
      "3  551 -0.851273  0.733108 -0.843535 -0.786363 -0.049836 -0.424532 -0.509221   \n",
      "4  199  0.091654  0.216499  0.103839 -0.034667  0.167930  0.308132  0.366614   \n",
      "\n",
      "         x7        x8  ...       x10       x11       x12       x13       x14  \\\n",
      "0  0.268537  0.015996  ... -0.337360 -0.728193 -0.442587 -0.272757 -0.608018   \n",
      "1 -0.898898  0.983496  ... -0.493639  0.348620 -0.552483 -0.526877  2.253098   \n",
      "2 -1.261820 -0.327193  ... -0.666881 -0.779358 -0.708418 -0.637545  0.710369   \n",
      "3 -0.679649  0.797298  ... -0.451772  0.453852 -0.431696 -0.494754 -1.182041   \n",
      "4  0.280661  0.505223  ... -0.707304 -1.026834 -0.702973 -0.460212 -0.999033   \n",
      "\n",
      "        x15       x16       x17       x18       x19  \n",
      "0 -0.577235 -0.501126  0.143371 -0.466431 -0.554102  \n",
      "1 -0.827620 -0.780739 -0.376997 -0.310239  0.176301  \n",
      "2 -0.976454 -1.057501 -1.913447  0.795207 -0.149751  \n",
      "3  0.281228  0.084759 -0.252420  1.038575  0.351054  \n",
      "4 -0.531406 -0.394360 -0.728830 -0.644416 -0.688003  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dense_df = pd.read_csv(dense_data_dir)\n",
    "print(dense_df.isna().sum())\n",
    "print(dense_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上傳資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/projects/python/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " UPLOADING:||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.00%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-06-06 05:34:05.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mJob id is 202306060534058190480\n",
      "\u001b[0m\n",
      "\u001b[32m2023-06-06 05:34:05.981\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KJob is still waiting, time elapse: 0:00:00\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-06-06 05:34:06.990\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KJob is still waiting, time elapse: 0:00:01\u001b[0m\n",
      "\u001b[0mm2023-06-06 05:34:08.007\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1m\n",
      "\u001b[32m2023-06-06 05:34:08.008\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component upload_0, time elapse: 0:00:02\u001b[0m\n",
      "\u001b[32m2023-06-06 05:34:09.024\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component upload_0, time elapse: 0:00:03\u001b[0m\n",
      "\u001b[32m2023-06-06 05:34:10.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component upload_0, time elapse: 0:00:04\u001b[0m\n",
      "\u001b[32m2023-06-06 05:34:11.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component upload_0, time elapse: 0:00:05\u001b[0m\n",
      "\u001b[32m2023-06-06 05:34:12.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component upload_0, time elapse: 0:00:06\u001b[0m\n",
      "\u001b[32m2023-06-06 05:34:13.100\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component upload_0, time elapse: 0:00:07\u001b[0m\n",
      "\u001b[32m2023-06-06 05:34:14.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component upload_0, time elapse: 0:00:08\u001b[0m\n",
      "\u001b[32m2023-06-06 05:34:15.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component upload_0, time elapse: 0:00:09\u001b[0m\n",
      "\u001b[32m2023-06-06 05:34:16.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mJob is success!!! Job id is 202306060534058190480\u001b[0m\n",
      "\u001b[32m2023-06-06 05:34:16.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mTotal time: 0:00:10\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pipeline.backend.pipeline import PipeLine\n",
    "pipeline_upload = PipeLine().set_initiator(role='guest', party_id=guest).set_roles(guest=guest)\n",
    "partition = 4\n",
    "\n",
    "pipeline_upload.add_upload_data(file=dense_data_dir,\n",
    "                                table_name=dense_data[\"name\"],             # table name\n",
    "                                namespace=dense_data[\"namespace\"],         # namespace\n",
    "                                head=1, partition=partition)               # data info\n",
    "pipeline_upload.upload(drop=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## two_party_hybrid_input_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 設定資料路徑 & 參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "guest, host = 9999, 10000\n",
    "data_base = \"/data/projects/fate/\"\n",
    "\n",
    "dense_data = {\"name\": \"titanic_hetero_host\", \"namespace\": f\"experiment\"}\n",
    "dense_data_dir = os.path.join(data_base, \"persistence/data/titanic_hetero_host.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缺失值 & 欄位名\n",
    "\n",
    "#### 欄位名請全部調整成小寫\n",
    "\n",
    "#### embarked 欄位有缺值 Data Transform 時要設定補缺值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passengerid    0\n",
      "parch          0\n",
      "fare           0\n",
      "embarked       2\n",
      "dtype: int64\n",
      "   passengerid  parch     fare embarked\n",
      "0            1      0   7.2500        S\n",
      "1            2      0  71.2833        C\n",
      "2            3      0   7.9250        S\n",
      "3            4      0  53.1000        S\n",
      "4            5      0   8.0500        S\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dense_df = pd.read_csv(dense_data_dir)\n",
    "print(dense_df.isna().sum())\n",
    "print(dense_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['S', 'C', 'Q', nan], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_df.embarked.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上傳資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " UPLOADING:||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.00%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-06-06 05:43:00.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mJob id is 202306060543003934990\n",
      "\u001b[0m\n",
      "\u001b[32m2023-06-06 05:43:00.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KJob is still waiting, time elapse: 0:00:00\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0mm2023-06-06 05:43:01.560\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1m\n",
      "\u001b[32m2023-06-06 05:43:01.562\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component upload_0, time elapse: 0:00:01\u001b[0m\n",
      "\u001b[32m2023-06-06 05:43:02.579\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component upload_0, time elapse: 0:00:02\u001b[0m\n",
      "\u001b[32m2023-06-06 05:43:03.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component upload_0, time elapse: 0:00:03\u001b[0m\n",
      "\u001b[32m2023-06-06 05:43:04.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component upload_0, time elapse: 0:00:04\u001b[0m\n",
      "\u001b[32m2023-06-06 05:43:05.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component upload_0, time elapse: 0:00:05\u001b[0m\n",
      "\u001b[32m2023-06-06 05:43:06.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component upload_0, time elapse: 0:00:06\u001b[0m\n",
      "\u001b[32m2023-06-06 05:43:07.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component upload_0, time elapse: 0:00:07\u001b[0m\n",
      "\u001b[32m2023-06-06 05:43:08.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component upload_0, time elapse: 0:00:08\u001b[0m\n",
      "\u001b[32m2023-06-06 05:43:09.679\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mJob is success!!! Job id is 202306060543003934990\u001b[0m\n",
      "\u001b[32m2023-06-06 05:43:09.680\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mTotal time: 0:00:09\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pipeline.backend.pipeline import PipeLine\n",
    "pipeline_upload = PipeLine().set_initiator(role='guest', party_id=guest).set_roles(guest=guest)\n",
    "partition = 4\n",
    "\n",
    "pipeline_upload.add_upload_data(file=dense_data_dir,\n",
    "                                table_name=dense_data[\"name\"],             # table name\n",
    "                                namespace=dense_data[\"namespace\"],         # namespace\n",
    "                                head=1, partition=partition)               # data info\n",
    "pipeline_upload.upload(drop=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad4309918fa4cd1705b305e369b2f64d901b1851e9144aef7b9b07ea3efcb1bb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
